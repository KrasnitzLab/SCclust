---
title: "SCclust T10 Tutorial"
author: "Alex Krasnitz, Jude Kendall, Junyan Song, Lubomir Chorbadjiev"
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: yes
    toc: yes
  html_document:
    df_print: paged
    toc: yes
header-includes:
- \usepackage[colorinlistoftodos]{todonotes}
- \hypersetup{colorlinks=true,linkcolor=blue!30!black}
- \geometry{rmargin=1.5in}
vignette: |
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
devtools::load_all("..")
```

# Introduction

The *SCclust* package performs phylogenetic analysis for sets of integer-valued genomic DNA copy-number profiles, each representing
a single nucleus.
For this purpose,*SCclust* determines a joint set of change points present in the profiles and uses these as features.
An incidence table is then set up, with profiles as columns and features as rows and the binary values indicating, for each profile
and feature, the presence of the feature in the profile. Dissimilarities among the profiles are computed as Fisher test p-values 
for pairs of columns. For these, false discovery rates (FDR) are computed using permutations within the table. A hierarchical tree is derived from the dissimilarity matrix, and its branches are identified as clones or sub-clones depending on the maximal dissimilarity and on the number of features shared within the branch. 

In this tutorial we show how to use *SCclust* package using data, prepared by
*sgains* pipeline as described in 
[Example usage of sGAINS pipeline](https://github.com/KrasnitzLab/sgains/blob/master/docs/tutorial-navin2011.md). 
*SCclust* package is called as the last step in processing data from *sgains* pipeline. In this tutoral
we show how *SCclust* package could be used independently from *sgains* pipeline.

We assume that you have an R environment and have installed *SCclust* package as described in the `README.md`.

# Data

## Data for the T10 case

This tutorial is based on data published in:
[Navin N, Kendall J, Troge J, et al. Tumor Evolution Inferred by Single 
Cell Sequencing. 
Nature. 2011;472(7341):90-94. doi:10.1038/nature09807.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4504184/)
In particular we will use single-cell data for polygenomic breast tumor T10 case available from SRA.
Description of samples for T10 could be found in
[Supplementary Table 1 | Summary of 100 Single Cells in the Polygenomic Tumor 
T10](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4504184/bin/NIHMS706545-supplement-Supplement_Table_1.xls)

Here we will execute *SCclust* package on input prepared by *sgains* pipeline [`varbin` step](https://github.com/KrasnitzLab/sgains/blob/master/docs/tutorial-navin2011.md#varbin-step).
Alternatively, the reader can go through the data pre-processing steps following [*sgains* T10 tutorial](https://github.com/KrasnitzLab/sgains/blob/master/docs/tutorial-navin2011.md).
For the purposes of this tutorial we recommend that you download already prepared `varbin` data from 
[example data](https://github.com/KrasnitzLab/SCclust/releases/download/v1.0.0RC3/navin_t10_varbin_data.tar.gz).
For convenience, we briefly describe the `varbin` data format later in this tutorial.
Apart from `varbin` T10 data you will need the binning scheme used in the analysis, which could be found [here](https://github.com/KrasnitzLab/SCclust/releases/download/v1.0.0RC3/hg19_R50_B20k_bins_boundaries.txt.gz).
Finally, we will need `cytoBand.txt`, the cytoband annotation file for the HG19 version of the human genome, which can be downloaded
from the UCSC Genome Browser. 

## Collect the Neccessary Data

We first create a directory where the data used in this tutorial will be stored:

```bash
mkdir T10data
cd T10data
```

Next, we download and extract T10 `varbin` data:

```bash
wget -c \
  https://github.com/KrasnitzLab/SCclust/releases/download/v1.0.0RC3/\
  navin_t10_varbin_data.tar.gz
tar zxvf navin_t10_varbin_data.tar.gz
rm navin_t10_varbin_data.tar.gz
```

We also download and extract the binning scheme used in preparation of `varbin` data:

```bash
wget -c \
  https://github.com/KrasnitzLab/SCclust/releases/download/v1.0.0RC3/\
  hg19_R50_B20k_bins_boundaries.txt.gz
gunzip hg19_R50_B20k_bins_boundaries.txt.gz
```

And finally we download the `cytoBand.txt`, a cytoband annotation table for Human reference genome *hg19*:

```bash
wget -c \
  http://hgdownload.cse.ucsc.edu/goldenPath/hg19/database/cytoBand.txt.gz
gunzip cytoBand.txt.gz
```

The data directory should have following structure:

```bash
.
|-- T10data
    |-- cytoBand.txt
    |-- hg19_R50_B20k_bins_boundaries.txt
    |-- varbin
        |-- SRR052047.varbin.20k.txt
        |-- SRR052148.varbin.20k.txt
        |-- SRR053437.varbin.20k.txt
        ...
```

## Explore the Downloaded Data

Next, we attach *SCclust* to the R session:

```{r}
library("SCclust")
```

### Binning scheme

The binning scheme is a table describing a partition of the genome into bins to sort the sequence reads into. The bin boundaries 
are defined such that each bin contains an approximately equal number of uniquely mapping reads. Also tabulated is the GC content of 
each bin.

```{r, results = "asis"}
gc_df <- read.csv("T10data/hg19_R50_B20k_bins_boundaries.txt", 
                  header = T, sep='\t')
gc_df$chrom.numeric <- chrom_numeric(gc_df$bin.chrom)
pander::pandoc.table(head(gc_df))
```

We are using a binning scheme with 20000 bins:

```{r, results = "asis"}
dim(gc_df)
```



### Cytobands and Centromeres for HG19

The `cytoBand.txt` file contains a table of cytoband boundary coordinates as per HG19.

```{r, results = "asis"}
cytobands <- read.csv("T10data/cytoBand.txt", header = F, sep='\t')
knitr::kable(head(cytobands))
```

The main reason we need `cytoBand.txt` is to get the location of centromeres. Centromeric regions of the genome often 
contain repetitive sequence, leading to mapping artefacts and distorted read counts. For this reason, we opt to mask them 
prior to copy-number profile segmentation. In the present version we mask cytobands p11 through q11.

To identify centromeric regions to be masked we use `calc_centroareas` function passing the list of bands to be masked:

```{r, results = "asis"}
centroareas <- calc_centroareas(cytobands, centromere=c("p11", "q11"))
knitr::kable(head(centroareas))
```


The locations of centromeric regions are now tabulated in `centroareas`.

### Varbin Samples Data

The `SGAINS` pipeline produces separate `varbin` file for each sample. Each 'varbin' file is a comma-separated table with 
each row representing a bin and with columns for chromosome,
position of the bin in the chromosome, absolute position of the bin, how many reads are mapped in this bin and the ratio. Here 
the absolute position means the position relative to the start of chromosome 1, assuming that sequences of chromosomes 1,2,...,Y are
concatenated.

```{r, results = "asis"}
sample_df <- read.csv("T10data/varbin/SRR052047.varbin.20k.txt", 
                      header=T, sep='\t')
knitr::kable(head(sample_df))
```

```{r, results = "asis"}
sample_df <- read.csv("T10data/varbin/SRR052148.varbin.20k.txt", 
                      header=T, sep='\t')
knitr::kable(head(sample_df))
```

# Segmentation of Varbin Data

## Prepare list of bins that are inside or intersect with centromeric regions


Next, we use the `calc_regions2bins` function to determine which bins overlap with regions tabulated in `centroareas`:

```{r, results = "asis"}
centrobins <- calc_regions2bins(gc_df, centroareas)
length(centrobins)
```

## Exclude centromeric bins from the binning scheme

After excluding centromeric bins from the binning scheme, the new binning scheme has fewer bins:

```{r, results = "asis"}
gc_df <- gc_df[-centrobins, ]
dim(gc_df)
```

## Collect Varbin files for all samples

With the help of `varbin_input_files` we create a vector of character strings, each representing a path to a `varbin` data file 
in `T10data/varbin` directory:

```{r, results = "asis"}
varbin_files <- varbin_input_files("T10data/varbin", "*.varbin.20k.txt")
knitr::kable(head(varbin_files))
```

For the purpose of this tutorial we will use a subset of the first 10 samples out of a 100 total
found by `varbin_input_files` function:

```{r}
varbin_files <- varbin_files[seq(10),]
dim(varbin_files)
```

## Segment varbin files

Next, we approximate the copy number ratio as a function of the bin number by a piecewise-constant function. This process is called 
segmentation. First we normalize bincount based on the GC content using LOWESS smoothing and after that we use Circular Binary 
Segmentation (CBS) algorithm, as implemented in the R package `DNAcopy`, but with two important enhancements. 
First, very short segments are eliminated more consistently, such that they are guaranteed to be absent from the output segment table.
Secondly, single-cell copy number is integer everywhere in the genome. We use this property to estimate the absolute copy number: the 
segmented copy-number ratio is multiplied by a factor, chosen such that the result is as close as possible to an integer, on average
throughout the genome.

```{r}
res <- segment_varbin_files(varbin_files, gc_df, centrobins)
```

The `segment_varbin_files` function returns the segmented copy number ratio along with the input ratios for all the samples

```{r segments-data, results = "asis"}
knitr::kable(head(res$seg[,c(1:8)]))
```

```{r ratio-data, results = "asis"}
knitr::kable(head(res$ratio[,c(1:8)]))
```

## Construct file names for the segmented output and store segmentation and ratio results.

The file names as constructed will be required for visualization.
```{r case-filenames}
filenames <- case_filenames("T10data/results", "NavinT10")
```

```{r}
save_table(filenames$seg, res$seg)
save_table(filenames$ratio, res$ratio)
```



```{r}
cells <- uber_cells(res$seg)$cells
save_table(filenames$cells, data.frame(cell=cells))
```


```{r}
dir("T10data/results")
```


# Construct features and feature matrix

At this point, we convert the set of segmented copy number profiles into a feature matrix. This process includes a number of steps. 
First, since single-cell copy number must be integer, we round the segmented values to the nearest integer. Some of the neighboring 
segments may merge as a result. As part of this procedure we calculate the ploidy of each cell. Ploidy here means the predominant 
integer value of copy number in the cell genome. We compute this 
value for three alternative definitions of ploidy: the genome-wide median (`ploidymed`), the genome-wide mode (`ploidymod`) and the mode 
of all chromosome-widemodes (`ploidychromod`). Also, at this point we compute the percentage of the genome with copy number 0, i.e., lost 
homozygously (`homoloss`). A high percentage of homozygous loss is likely a sign of DNA degradation ex-vivo. Such cells are removed from 
further processing (this is controlled using `homoloss` parameter of `calc_pinmat` fuction).

```{r ploidis-data, results = "asis"}
ploidies_df <- calc_ploidies(gc_df, res$seg)
knitr::kable(head(ploidies_df))
```

Next, we replace each profile by a set of change points, i.e., discontinuities in the integer segmented
value. Each change point has two values associated with it: position (bin number) and the sign of change going in the direction of
increasing bin number. The positive and negative change points are handled separately.  Further, we turn change points into short 
intervals centered at the change point positions, with the interval length is an integer number of bin sdetermined by the `smear` 
parameter. For each sign of the change points, the features are a minimal set of points such that at least one of them is contained 
in each of the short intervals centered at the change points. The feature matrix  is then a binary matrix with cells as columns 
and features as rows, indicating, for each cell and feature, the presence of a short change-point centered interval containing the 
feature. The results are stored in a list called `pins`, with two items: `pins` is a tabulation of the feature locations and signs and
`pinmat` is the feature matrix.

```{r featuremat}
pins <- calc_pinmat(gc_df, res$seg, dropareas=centroareas)
pinmat_df <- pins$pinmat
pins_df <- pins$pins
save_table(filenames$featuremat, pinmat_df)
save_table(filenames$features, pins_df)
```

```{r}
dir("T10data/results")
```


# Dissimilarity matrix and significance of dissimilarities

In the next step, we use the feature matrix to compute dissimilarities among cells. These are defined, for each pair of cells,
as (one-sided) Fisher test p-values for the corresponding two columns of the feature matrix. To analyze the significance of 
dissimilarities, we will need to compare their observed distribution to a sampling from a null model. For the latter, we perform `nsim` 
permutations of the feature set, preserving the column and, approximately, the row sums and recomputing the dissimilarity matrix each 
time. Each permutation consists of `nsweep` sweeps. A sweep consists of a Metroplolis-Hastings step for every row and a swapping move 
for pairs of columns, such that every column is involved. Positive and negative feature submatrices are permuted separately. The entire 
procedure is implemented as `sim_fisher_wrapper` function, which returns a list, with the vectors of the observed dissimilarities and 
those sampled from the null distribution as items.

```{r simfisher}
fisher <- sim_fisher_wrapper(
      pinmat_df, pins_df, njobs=30, nsim=150, nsweep=10)
true_pv <- fisher$true
sim_pv <- fisher$sim
```

These distributuions can be saved if so desired.

```{r}
save_mat(filenames$true_pv, true_pv)
save_mat(filenames$sim_pv, sim_pv)
```

Next, we use function `fisher_fdr` to determine which of the observed dissimilarities are outliers on the left in the null distribution, 
i.e., much smaller than expected. The value is a matrix of false discovery rates (FDR) for every pair of cells. We also arrange the 
observed dissimilarities in a distance matrix.

```{r fisher-fdr}
mfdr <- fisher_fdr(true_pv, sim_pv, cells)
mfdr[1:5,1:5]
mdist <- fisher_dist(true_pv, cells)
mdist[1:5,1:5]
```


# Hierarchical clustering

At this point, we use the distance matrix derived as above to compute hierarchical clustering of cells. The function `hclust_tree`
invokes the R core function `hclust` with the linkage choice as indicated (here `'average'`, the default value) and with `mdist` as the
dissimilarity matrix. The value is an extension of the `R` `hclust` object, to include additional items, as required for the clone
identification in the following. The more important ones are `nodesize` (the number of leaves for each node), `mergefdr` (maximal
pairwise FDR anywhere in the node) and `sharing`, a matrix specifying for each node and each feature the fraction of leaves in the node
with the feature. As an interface with Python software, the function `tree_py` is provided, which outputs the same hierarchical tree in 
the format for Python hierarchical clustering suite.

```{r clustering-tree}
hc <- hclust_tree(pinmat_df, mfdr, mdist,hcmethod='average')
names(hc)
tree_df <- tree_py(mdist, method='average')
head(tree_df)
```


```{r}
save_table(filenames$tree, tree_df)
```


# Finding clones and subclones

Finally, we determine which nodes of the tree represent clones. Two types of clones are considered: hard and soft. A hard clone is 
defined as a node with at least `minsize` leaves for which `mergefdr` is below `fdrthresh` (the FDR condition), the number of features 
shared by at least `sharemin` 
leaves is at least `nshare` (the sharing condition), and such that the parent node does not have at least one of these two properties. 
Once the hard clones are identified, they can be expanded as follows: ask whether the parent of the hard clone meets the sharing 
condition as above; if not, the hard clone is also the soft clone; if so, expand the clone to the parent and iterate. 
```{r clones}
hc <- find_clones(hc)
```

The suction `find_clones` supports multiple parameters that control its behavior such as 
`fdrthresh` that defines maximal allowed value for log10(FDR) for any pair of leaves in a clone node,
`sharemin` that defines when a feature is considered 'widely shared' if present in sharemin fraction of leaves in a node,
`nshare` that defines minimal number of 'widely shared' features in a hard clone. For the full list of parameters please consult
the manual of SCclust package.



As a result of this computation, an item called `softclones` is added to `hc`. It is a matrix with two rows named `hard` and `soft` and 
as many columns as there are hard clones. In the `hard` row the node numbers for all hard clones are listed, whereas the `soft` row contains 
the node numbers for the soft clones obtained by expanding each of the hard clones. An expansion from distinct hard clones may result in 
the same soft clone.
```{r}
hc$softclones
```

To complete the analysis, we compute subclones. In essence, this computation is a loop over clones. If the number of cells in the clone
exceeds a minimal value (e.g., 6), we extract a submatrix of the feature matrix, restricted to the cells in the clone. The clonal structure analysis is then performed for this sub-matrix as described above, starting with the computation of dissimilarities. 
```{r subclones}
subclones <- find_subclones(hc, pinmat_df, pins_df, nsim=nsim)
```

For the full list of parameters that control the behavior of `find_sublones` please consult the manual pages of SCclust package.

```{r}
save_table(filenames$clone, subclones)
```
